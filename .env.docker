# Docker Environment Configuration for Ollama WebUI
# Copy this file to .env and modify the values according to your Docker environment

# Ollama Configuration
# Note: Use host.docker.internal to access Ollama running on the host machine
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Model Configuration
DEFAULT_MODEL=llama3.2:3b
DEFAULT_TEMPERATURE=0.5
DEFAULT_TIMEOUT=60

# Application Security (IMPORTANT: Change for production)
DEBUG=False
ALLOWED_HOSTS=localhost,127.0.0.1,frontend
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# Server Configuration
GUNICORN_PORT=8000
GUNICORN_WORKERS=4

# Frontend Configuration
VITE_OLLAMA_SERVER_BASE_URL=http://localhost:8000

# Development Configuration (uncomment for development mode)
# DEBUG=True
# ALLOWED_HOSTS=*
# CORS_ORIGINS=*

# Production Configuration Notes:
# - Never use * in ALLOWED_HOSTS or CORS_ORIGINS in production
# - Consider using a reverse proxy like Nginx in production
# - Use proper SSL certificates for HTTPS
# - Set appropriate resource limits for containers
# - Use secrets management for sensitive data