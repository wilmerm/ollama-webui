# Ollama WebUI

Ollama WebUI es una interfaz gr谩fica web minimalista y f谩cil de usar, desarrollada con FastAPI y Vue.js, que permite interactuar con modelos de IA locales a trav茅s de Ollama.

![image](https://github.com/user-attachments/assets/fbce0ca0-e4a6-4f93-a102-079d05ae3c25)


## Caracter铆sticas

- Interfaz intuitiva y amigable
- Backend r谩pido con FastAPI
- Interfaz interactiva creada con Vue.js
- Soporte para m煤ltiples modelos de IA locales
-  **Soporte completo para Docker**

## M茅todos de Instalaci贸n

###  Opci贸n 1: Docker (Recomendado)

La forma m谩s f谩cil de ejecutar Ollama WebUI es utilizando Docker. Esta opci贸n no requiere instalaci贸n manual de dependencias.

#### Requisitos Previos
- Docker y Docker Compose instalados
- Ollama ejecut谩ndose en tu sistema (puerto 11434)

#### Inicio R谩pido con Docker

**Opci贸n A: Script de instalaci贸n autom谩tica (Recomendado)**

```bash
# Clona el repositorio
git clone https://github.com/wilmerm/ollama-webui.git
cd ollama-webui

# Ejecuta el script de setup
./docker-setup.sh
```

**Opci贸n B: Instalaci贸n manual**

1. **Clona el repositorio:**
   ```bash
   git clone https://github.com/wilmerm/ollama-webui.git
   cd ollama-webui
   ```

2. **Configura las variables de entorno:**
   ```bash
   cp .env.docker .env
   ```

3. **Ejecuta con Docker Compose:**
   ```bash
   docker compose up -d
   ```

4. **Accede a la aplicaci贸n:**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000

#### Configuraci贸n Docker

La configuraci贸n predeterminada funciona con Ollama ejecut谩ndose en el sistema host. Aseg煤rate de que:

- Ollama est茅 ejecut谩ndose: `ollama serve`
- El modelo est茅 disponible: `ollama pull llama3.2:3b`

**Variables de entorno importantes para Docker:**

```env
# Conexi贸n a Ollama en el host
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Configuraci贸n del modelo
DEFAULT_MODEL=llama3.2:3b

# Puertos de la aplicaci贸n
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
```

#### Comandos Docker tiles

```bash
# Iniciar en segundo plano
docker compose up -d

# Ver logs
docker compose logs -f

# Reconstruir contenedores
docker compose build

# Detener contenedores
docker compose down

# Para producci贸n (con optimizaciones adicionales)
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

#### Desarrollo con Docker

Para desarrollo, puedes montar vol煤menes para cambios en tiempo real:

```yaml
# Descomenta estas l铆neas en docker-compose.yml para desarrollo
volumes:
  - ./backend:/app/backend:ro
  - ./frontend/vue-app/src:/app/src:ro
```

###  Opci贸n 2: Instalaci贸n Manual

#### Requisitos
- Python 3.8+
- Node.js 16+
- Ollama instalado y ejecut谩ndose

#### Pasos de Instalaci贸n

1. Clona el repositorio:
    ```bash
    git clone https://github.com/wilmerm/ollama-webui.git
    cd ollama-webui
    ```

2. Instala las dependencias del backend:
    ```bash
    pip install -r requirements.txt
    ```

3. Instala las dependencias del frontend:
    ```bash
    cd frontend/vue-app
    npm install
    ```

## Configuraci贸n de Variables de Entorno

### Para Docker

Si usas Docker, copia el archivo de ejemplo espec铆fico para Docker:

```bash
cp .env.docker .env
```

### Para Instalaci贸n Manual

**锔 Importante**: Copia el archivo `.env.example` a `.env` y modifica los valores seg煤n tu entorno.

```bash
cp .env.example .env
```

### Variables de Entorno Disponibles

**Configuraci贸n b谩sica:**

```ini
# URL de Ollama (para Docker usa host.docker.internal)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo por defecto
DEFAULT_MODEL=llama3.2:3b
DEFAULT_TEMPERATURE=0.5
DEFAULT_TIMEOUT=60

# Seguridad (IMPORTANTE para producci贸n)
DEBUG=False
ALLOWED_HOSTS=localhost,127.0.0.1
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173  # Para desarrollo manual
# CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000  # Para Docker
```

**Variables opcionales:**

```ini
# Configuraci贸n del servidor
GUNICORN_PORT=8000
GUNICORN_WORKERS=4

# Frontend (solo para Docker)
VITE_OLLAMA_SERVER_BASE_URL=http://localhost:8000

# Para desarrollo manual
PYTHON_VENV=.pyenv
```

**锔 Seguridad**: 
- **Nunca** uses `*` en `ALLOWED_HOSTS` o `CORS_ORIGINS` en producci贸n
- **Nunca** commitees el archivo `.env` al repositorio
- Lee las [Pautas de Seguridad](SECURITY.md) antes del despliegue

## Uso

### Con Docker

```bash
# Iniciar (primera vez)
docker compose up -d

# Ver logs
docker compose logs -f

# Detener
docker compose down
```

Accede a:
- **Frontend**: http://localhost:3000
- **API Backend**: http://localhost:8000

### Instalaci贸n Manual

1. Inicia el servidor backend:
    ```bash
    uvicorn backend.main:app --reload
    ```

2. Inicia el servidor frontend (en otra terminal):
    ```bash
    cd frontend/vue-app
    npm run dev
    ```

Accede a:
- **Frontend**: http://localhost:5173
- **API Backend**: http://localhost:8000

### Producci贸n (Instalaci贸n Manual)

Para producci贸n con Gunicorn:
```bash
gunicorn -w 4 -k uvicorn.workers.UvicornWorker backend.main:app --bind 0.0.0.0:8000
```

O simplemente ejecuta el script de inicio:
```bash
./start.sh
```

## Arquitectura de los Contenedores

### Backend (FastAPI)
- **Base**: `python:3.12-slim`
- **Puerto**: 8000
- **Caracter铆sticas**: 
  - Optimizado para producci贸n con Gunicorn
  - Usuario no-root por seguridad
  - Health checks incluidos
  - Variables de entorno configurables

### Frontend (Vue.js + Nginx)
- **Build**: `node:20-alpine` (para construcci贸n)
- **Runtime**: `nginx:alpine` (para servir archivos)
- **Puerto**: 80 (mapeado a 3000 en host)
- **Caracter铆sticas**:
  - Build multistage para im谩genes ligeras
  - Configuraci贸n Nginx optimizada
  - Proxy autom谩tico de API al backend
  - Compresi贸n gzip habilitada
  - Headers de seguridad

### Networking
- Red interna: `ollama-network`
- Frontend se comunica con backend via proxy interno
- Backend accede a Ollama en el host via `host.docker.internal`

## Personalizaci贸n Docker

### Variables de Entorno Docker

```env
# Configuraci贸n de Ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434
DEFAULT_MODEL=llama3.2:3b

# Seguridad
ALLOWED_HOSTS=localhost,127.0.0.1,frontend
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# Performance
GUNICORN_WORKERS=4
```

### Docker Compose para Producci贸n

```bash
# Usar configuraci贸n optimizada para producci贸n
docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

La configuraci贸n de producci贸n incluye:
- L铆mites de recursos (CPU/memoria)
- Modo read-only para mayor seguridad
- Usuarios no-root
- Reinicio autom谩tico
- Optimizaciones de seguridad adicionales

## Soluci贸n de Problemas

### Docker

1. **Error de conexi贸n a Ollama:**
   ```bash
   # Verifica que Ollama est茅 ejecut谩ndose
   ollama serve
   
   # Verifica la conectividad desde el contenedor
   docker compose exec backend curl -f http://host.docker.internal:11434/api/tags
   ```

2. **Frontend no puede acceder al backend:**
   - Verifica que ambos contenedores est茅n en la misma red
   - Revisa la configuraci贸n del proxy en `nginx.conf`

3. **Problemas de permisos:**
   ```bash
   # Reconstruir con permisos correctos
   docker compose build --no-cache
   ```

### General

- Aseg煤rate de que Ollama est茅 ejecut谩ndose: `ollama serve`
- Verifica que el modelo est茅 disponible: `ollama pull llama3.2:3b`  
- Revisa los logs para errores: `docker compose logs -f` (Docker) o revisa la consola (manual)
- Verifica las variables de entorno en el archivo `.env`

## Contribuciones

隆Contribuciones son bienvenidas! Abre un issue o env铆a un pull request.

## Licencia

Este proyecto est谩 licenciado bajo la Licencia MIT.
